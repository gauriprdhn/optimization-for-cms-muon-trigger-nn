{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2875097b",
   "metadata": {
    "id": "2875097b"
   },
   "source": [
    "### Part - I: Importing Required Modules/ Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751381c",
   "metadata": {
    "id": "4751381c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "\n",
    "from nn_globals import *\n",
    "from dataset import muon_data_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, TerminateOnNaN, EarlyStopping\n",
    "from nn_evaluate import huber_loss, k_fold_validation\n",
    "from nn_training import lr_schedule\n",
    "from nn_pruning_module_support import loading_trained_model\n",
    "from nn_training import train_model\n",
    "from nn_training_pruned_model import (generate_layer_masks, \n",
    "                                      create_sparse_model)\n",
    "from nn_plotting import __generate_delta_plots__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8b6c7",
   "metadata": {
    "id": "d1b8b6c7"
   },
   "source": [
    "### Part- II: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa893cf",
   "metadata": {},
   "source": [
    "`DATAFILEPATH` variable to be edited to point to the NN inputs file's path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wo6ZNYZKRjK0",
   "metadata": {
    "id": "wo6ZNYZKRjK0"
   },
   "outputs": [],
   "source": [
    "x_train_displ, x_test_displ, y_train_displ, y_test_displ, dxy_train_displ, dxy_test_displ =  muon_data_split(filename = DATAFILEPATH, \n",
    "                                                                                                                reg_pt_scale=REG_PT_SCALE, \n",
    "                                                                                                                reg_dxy_scale=REG_DXY_SCALE, \n",
    "                                                                                                                test_size=TEST_SIZE,\n",
    "                                                                                                                nvariables = NVARIABLES,\n",
    "                                                                                                                nentries = NENTRIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc7c79",
   "metadata": {
    "id": "20bc7c79"
   },
   "source": [
    "### Part-III: Use the cells in this section for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754959ad",
   "metadata": {
    "id": "754959ad"
   },
   "outputs": [],
   "source": [
    "# cols = [\"dphi_1\",\"dphi_2\",\"dphi_3\",\"dphi_4\",\"dphi_5\",\"dphi_6\",\n",
    "#        \"dtheta_1\",\"dtheta_2\",\"dtheta_3\",\"dtheta_4\",\"dtheta_5\", \"dtheta_6\",\n",
    "#        \"bend_1\",\"bend_2\",\"bend_3\",\"bend_4\",\n",
    "#        \"track theta\"]\n",
    "\n",
    "# x = np.concatenate((x_train_displ,x_test_displ),axis=0)\n",
    "# y = np.concatenate((y_train_displ,y_test_displ),axis=0)\n",
    "# dxy = np.concatenate((dxy_train_displ,dxy_test_displ),axis=0)\n",
    "\n",
    "# corr_plot(x,y,dxy,columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ubbR2loeCbxL",
   "metadata": {
    "id": "ubbR2loeCbxL"
   },
   "source": [
    "### Part- III: Load and Account the Perf. for the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1ba56",
   "metadata": {
    "id": "56b1ba56"
   },
   "outputs": [],
   "source": [
    "baseline = loading_trained_model(filepath = \"./models/\",\n",
    "                                 model_filename = \"model\")\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nzWEKXR1Ctry",
   "metadata": {
    "id": "nzWEKXR1Ctry"
   },
   "outputs": [],
   "source": [
    "# Quantitative Evaluation\n",
    "k_fold_validation(model = baseline, \n",
    "          x = x_test_displ, \n",
    "          y = y_test_displ, \n",
    "          dxy = dxy_test_displ, \n",
    "          folds =1,\n",
    "          metric_type = \"MAE\")    \n",
    "k_fold_validation(model = baseline, \n",
    "          x = x_test_displ, \n",
    "          y = y_test_displ, \n",
    "          dxy = dxy_test_displ, \n",
    "          folds =1,\n",
    "          metric_type = \"RMSE\")   \n",
    "\n",
    "# Qualitative Evaluation:\n",
    "__generate_delta_plots__(model = baseline,\n",
    "                          x = x_test_displ,\n",
    "                          y = y_test_displ,\n",
    "                          dxy = dxy_test_displ,\n",
    "                          color = \"red\",\n",
    "                         alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wwhNvFOpEe1m",
   "metadata": {
    "id": "wwhNvFOpEe1m"
   },
   "source": [
    "### Part-IV: Build the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630161ab",
   "metadata": {
    "id": "630161ab"
   },
   "outputs": [],
   "source": [
    "# def run_iterative_pruning(baseline_model = None,\n",
    "#                           target_sparsity: float = 0.1, \n",
    "#                           pruning_fraction_step:float = 0.1,\n",
    "#                           training_params: list = []):\n",
    "    \n",
    "#     if target_sparsity > 1.0 or target_sparsity <= 0:\n",
    "#         print(\"INVALID value entered for target sparsity, it can only be in the range [0,1]\")\n",
    "#     if pruning_fraction_step > target_sparsity:\n",
    "#         print(\"INVALID value entered for pruning fraction, it has to be <= target_sparsity\")\n",
    "    \n",
    "#     # list of models, new pruned models get appended to it while training\n",
    "#     pruned_models, training_history = [],[]\n",
    "#     plot_colors = [\"red\",\"orange\",\"blue\",\"cyan\",\"purple\",\"green\",\"magenta\", \"salmon\"]\n",
    "#     init_sparsity = pruning_fraction_step\n",
    "#     i = 0\n",
    "#     while(init_sparsity <= target_sparsity):\n",
    "\n",
    "#         print(\"-----------------------------------------------------------------------------------------------\")\n",
    "#         print(\"-----------------------------------------------------------------------------------------------\")\n",
    "#         print(\"Currently pruning the model upto {} % of the baseline\".format(round(init_sparsity*100)))\n",
    "#         print(\"-----------------------------------------------------------------------------------------------\")\n",
    "#         print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "#         x_train_displ, x_test_displ, y_train_displ, y_test_displ, dxy_train_displ, dxy_test_displ =  muon_data_split(infile_muon_displ, \n",
    "#                                                                                                                        reg_pt_scale=reg_pt_scale, \n",
    "#                                                                                                                        reg_dxy_scale=reg_dxy_scale, \n",
    "#                                                                                                                        test_size=0.315)\n",
    "#         y_train_displ = np.abs(y_train_displ)\n",
    "#         y_test_displ = np.abs(y_test_displ)\n",
    "\n",
    "#         # training loop begins\n",
    "#         lr = training_params[i]['lr']\n",
    "#         clipnorm = training_params[i]['clipnorm']\n",
    "#         eps = training_params[i]['eps']\n",
    "#         momentum = training_params[i]['momentum']\n",
    "#         retrain_epochs = training_params[i]['epochs']\n",
    "#         retrain_batch_size = training_params[i]['batch_size']\n",
    "#         l1_reg = training_params[i]['l1_reg']\n",
    "#         l2_reg = training_params[i]['l2_reg']\n",
    "#         sparsity = init_sparsity\n",
    "        \n",
    "#         # define optimizer, callbacks here\n",
    "#         adam = Adam(lr=lr, clipnorm=clipnorm)\n",
    "#         lr_decay = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "#         terminate_on_nan = TerminateOnNaN()\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', \n",
    "#                                        min_delta=1e-5, \n",
    "#                                        patience=40, \n",
    "#                                        verbose=True,\n",
    "#                                        mode='min')\n",
    "#         curr_model = None\n",
    "#         if len(pruned_models) == 0:\n",
    "#             curr_model = baseline_model\n",
    "#         else:\n",
    "#             curr_model = pruned_models[-1]\n",
    "\n",
    "#         pruned_model = create_sparse_model(model = curr_model,\n",
    "#                                                input_dim = nvariables,\n",
    "#                                                output_dim = 2,\n",
    "#                                                k_sparsity = sparsity,\n",
    "#                                                bn_epsilon = eps,\n",
    "#                                                bn_momentum = momentum,\n",
    "#                                                l1_reg = l1_reg,\n",
    "#                                                l2_reg = l1_reg,\n",
    "#                                                kernel_initializer=\"glorot_uniform\",\n",
    "#                                                optimizer = adam)\n",
    "\n",
    "#         pruned_model, history = train_model(model = pruned_model,\n",
    "#                                             x = x_train_displ,\n",
    "#                                             y = np.column_stack((y_train_displ, dxy_train_displ)),\n",
    "#                                             epochs = retrain_epochs,\n",
    "#                                             batch_size = retrain_batch_size,\n",
    "#                                             callbacks=[lr_decay, \n",
    "#                                                       early_stopping, \n",
    "#                                                       terminate_on_nan],\n",
    "#                                             verbose = True,\n",
    "#                                             validation_split=0.1)\n",
    "        \n",
    "#         # evaluate the model\n",
    "#         k_fold_validation(model = pruned_model, \n",
    "#                   x = x_test_displ, \n",
    "#                   y = y_test_displ, \n",
    "#                   dxy = dxy_test_displ, \n",
    "#                   folds =1,\n",
    "#                   metric_type = \"MAE\")    \n",
    "#         k_fold_validation(model = pruned_model, \n",
    "#                   x = x_test_displ, \n",
    "#                   y = y_test_displ, \n",
    "#                   dxy = dxy_test_displ, \n",
    "#                   folds =1,\n",
    "#                   metric_type = \"RMSE\")   \n",
    "\n",
    "#         __generate_delta_plots__(model = pruned_model,\n",
    "#                                  x = x_test_displ,\n",
    "#                                  y = y_test_displ,\n",
    "#                                  dxy = dxy_test_displ,\n",
    "#                                  color = plot_colors[i])\n",
    "        \n",
    "#         pruned_models.append(pruned_model)\n",
    "#         training_history.append(history)\n",
    "\n",
    "#         # training ends\n",
    "#         i += 1\n",
    "#         init_sparsity += pruning_fraction_step\n",
    "    \n",
    "#     return pruned_models, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ntf330Vny7Yr",
   "metadata": {
    "id": "Ntf330Vny7Yr"
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "def run_iterative_pruning_v2 (X_train ,\n",
    "                              y_train ,\n",
    "                              dxy_train ,\n",
    "                              X_test ,\n",
    "                              y_test ,\n",
    "                              dxy_test ,\n",
    "                              baseline_model,\n",
    "                              pruning_type:str = \"unstructured\",\n",
    "                              init_sparsity: float = 0.1 ,\n",
    "                              target_sparsity: float = 1.0 ,\n",
    "                              pruning_fraction_step: float = 0.1 ,\n",
    "                              training_params: List[dict] = None ,\n",
    "                              cv_folds: int = 1 ,\n",
    "                              plot_colors: Union[str,List[str],None] = \"red\") :\n",
    "    \n",
    "    if target_sparsity > 1.0 or target_sparsity <= 0 :\n",
    "        print (\"INVALID value entered for target sparsity, it can only be in the range [0,1]\")\n",
    "    if pruning_fraction_step > target_sparsity :\n",
    "        print (\"INVALID value entered for pruning fraction, it has to be <= target_sparsity\")\n",
    "\n",
    "    # list of models, new pruned models get appended to it while training\n",
    "    pruned_models , training_historys = [] , []\n",
    "    i = 0\n",
    "    while (init_sparsity <= target_sparsity) :\n",
    "\n",
    "        print (\"-----------------------------------------------------------------------------------------------\")\n",
    "        print (\"-----------------------------------------------------------------------------------------------\")\n",
    "        print (\"Currently pruning the model upto {} % of the baseline\".format (round (init_sparsity * 100)))\n",
    "        print (\"-----------------------------------------------------------------------------------------------\")\n",
    "        print (\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # training loop begins\n",
    "        lr = training_params [i] ['lr']\n",
    "        clipnorm = training_params [i] ['clipnorm']\n",
    "        eps = training_params [i] ['eps']\n",
    "        momentum = training_params [i] ['momentum']\n",
    "        retrain_epochs = training_params [i] ['epochs']\n",
    "        retrain_batch_size = training_params [i] ['batch_size']\n",
    "        l1_reg = training_params [i] ['l1_reg']\n",
    "        l2_reg = training_params [i] ['l2_reg']\n",
    "        sparsity = init_sparsity\n",
    "\n",
    "        # define optimizer, callbacks here\n",
    "        adam = Adam (lr=lr , clipnorm=clipnorm)\n",
    "        lr_decay = LearningRateScheduler (lr_schedule , verbose=1)\n",
    "        terminate_on_nan = TerminateOnNaN ()\n",
    "        early_stopping = EarlyStopping (min_delta=1e-5 ,\n",
    "                                        monitor='val_loss' ,\n",
    "                                        patience=10 ,\n",
    "                                        verbose=True ,\n",
    "                                        mode='auto')\n",
    "        curr_model = None\n",
    "        if len (pruned_models) == 0 :\n",
    "            curr_model = baseline_model\n",
    "        else :\n",
    "            curr_model = pruned_models [-1]\n",
    "\n",
    "        pruned_model = create_sparse_model (model=curr_model ,\n",
    "                                            input_dim=NVARIABLES ,\n",
    "                                            output_dim=2 ,\n",
    "                                            k_sparsity=sparsity ,\n",
    "                                            pruning_type=pruning_type,\n",
    "                                            bn_epsilon=eps ,\n",
    "                                            bn_momentum=momentum ,\n",
    "                                            l1_reg=l1_reg ,\n",
    "                                            l2_reg=l1_reg ,\n",
    "                                            kernel_initializer=\"glorot_uniform\",\n",
    "                                            optimizer=adam)\n",
    "\n",
    "        pruned_model , history = train_model (model=pruned_model ,\n",
    "                                              x=X_train ,\n",
    "                                              y=np.column_stack ((y_train , dxy_train)) ,\n",
    "                                              epochs=retrain_epochs ,\n",
    "                                              batch_size=retrain_batch_size ,\n",
    "                                              callbacks=[lr_decay ,\n",
    "                                                         early_stopping ,\n",
    "                                                         terminate_on_nan] ,\n",
    "                                              verbose=True ,\n",
    "                                              validation_split=0.1)\n",
    "\n",
    "        k_fold_validation (model=pruned_model ,\n",
    "                           x=X_test ,\n",
    "                           y=y_test ,\n",
    "                           dxy=dxy_test ,\n",
    "                           folds=cv_folds ,\n",
    "                           metric_type=\"MAE\")\n",
    "        k_fold_validation (model=pruned_model ,\n",
    "                           x=X_test ,\n",
    "                           y=y_test ,\n",
    "                           dxy=dxy_test ,\n",
    "                           folds=cv_folds ,\n",
    "                           metric_type=\"RMSE\")\n",
    "        if isinstance(plot_colors, list):\n",
    "            plot_color = plot_colors[i]\n",
    "        else:\n",
    "            plot_color = plot_colors\n",
    "        __generate_delta_plots__ (model=pruned_model ,\n",
    "                                  x=X_test ,\n",
    "                                  y=y_test ,\n",
    "                                  dxy=dxy_test ,\n",
    "                                  color=plot_color,\n",
    "                                  bins_y = [-3.,3.],\n",
    "                                  bins_dxy = [-75.,75.])\n",
    "\n",
    "        pruned_models.append (pruned_model)\n",
    "        training_historys.append (history)\n",
    "\n",
    "        # training ends\n",
    "        i += 1\n",
    "        init_sparsity += pruning_fraction_step\n",
    "\n",
    "    return pruned_models , training_historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a61c8a",
   "metadata": {
    "id": "31a61c8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = [\"salmon\",\"plum\",\"green\",\"purple\",\"blue\",\"orange\", \"turquoise\"]\n",
    "pruned_models , training_historys = run_iterative_pruning_v2(x_train_displ, \n",
    "                                                             y_train_displ, \n",
    "                                                             dxy_train_displ,\n",
    "                                                             x_test_displ, \n",
    "                                                             y_test_displ, \n",
    "                                                             dxy_test_displ,\n",
    "                                                             baseline_model = baseline,\n",
    "                                                             init_sparsity = 0.1,\n",
    "                                                             target_sparsity = 0.7,\n",
    "                                                             pruning_fraction_step = 0.1,\n",
    "                                                             training_params = FT_PARAMS,\n",
    "                                                             cv_folds = 1,\n",
    "                                                             plot_colors = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83027a",
   "metadata": {
    "id": "8d83027a"
   },
   "source": [
    "### Part- V: Saving and Loading the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a376f3",
   "metadata": {
    "id": "44a376f3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nn_pruning_module_support import saving_model\n",
    "for i in range(len(pruned_models)):\n",
    "    j = (i+1)*10\n",
    "    with open(sys.path[-1] +'/' + \"trainingLog_US_\" + str(j), 'wb') as file_pi:\n",
    "        pickle.dump(training_history[i].history, file_pi)\n",
    "    model_filename = \"custom_model_US_\" + str(j)\n",
    "    saving_model(model = pruned_models[i], \n",
    "                      filepath=  \"./models\", \n",
    "                      model_filename = model_filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "implementing_iterative_pruning-v2.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
