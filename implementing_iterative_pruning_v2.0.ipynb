{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2875097b",
   "metadata": {
    "id": "2875097b"
   },
   "source": [
    "### Part - I: Importing Required Modules/ Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4751381c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4751381c",
    "outputId": "cafc72ad-cbd6-44d8-f742-6860ad046c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n",
      "1.3.4\n",
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "import pickle\n",
    "\n",
    "# custom declarations \n",
    "from nn_globals import *\n",
    "from nn_plotting import gaus, fit_gaus, corr_plot\n",
    "from dataset import muon_data_split\n",
    "from nn_evaluate import huber_loss, k_fold_validation\n",
    "from nn_training import lr_schedule\n",
    "from nn_pruning_module_support import loading_trained_model\n",
    "from nn_training_pruned_model import (generate_layer_masks, \n",
    "                                      create_sparse_model,\n",
    "                                      train_sparse_model)\n",
    "from nn_pruning_module_support import __generate_delta_plots__\n",
    "from nn_pruning_module_support import (saving_pruned_model,loading_pruned_model)\n",
    "fron nn_evaluate import get_sparsity\n",
    "from custom_dense_layer import MaskedDense\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (LearningRateScheduler, \n",
    "                                        TerminateOnNaN, \n",
    "                                        EarlyStopping)\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8b6c7",
   "metadata": {
    "id": "d1b8b6c7"
   },
   "source": [
    "### Part- II: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wo6ZNYZKRjK0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wo6ZNYZKRjK0",
    "outputId": "cd4f00d0-18f0-45ce-b1fd-234c0309358b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] Loading muon data from /Users/gpradhan/Desktop/optimization-for-cms-muon-trigger-nn/data/NN_input_params_FlatXYZ.npz ...\n",
      "[INFO    ] Loaded the variables with shape (19300000, 25)\n",
      "[INFO    ] Loaded the parameters with shape (19300000, 6)\n",
      "[INFO    ] Loaded the encoded variables with shape (3284620, 23)\n",
      "[INFO    ] Loaded the encoded parameters with shape (3284620,)\n",
      "[INFO    ] Loaded # of training and testing events: (2249964, 1034656)\n",
      "[WARNING ] The last batch for training could be too few! (2024967%128)=7. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*0.9) % 128\n",
      "[WARNING ] The last batch for training after mixing could be too few! (4049935%128)=15. Please change test_size.\n",
      "[WARNING ] Try this formula: int(int(3284620*0.685)*2*0.9) % 128\n"
     ]
    }
   ],
   "source": [
    "x_train_displ, x_test_displ, y_train_displ, y_test_displ, \\\n",
    "dxy_train_displ, dxy_test_displ =  muon_data_split(infile_muon_displ, \n",
    "                                                    reg_pt_scale=reg_pt_scale, \n",
    "                                                    reg_dxy_scale=reg_dxy_scale, \n",
    "                                                    test_size=0.315,\n",
    "                                                    batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc7c79",
   "metadata": {
    "id": "20bc7c79"
   },
   "source": [
    "### Part-III: Use the cells in this section for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754959ad",
   "metadata": {
    "id": "754959ad"
   },
   "outputs": [],
   "source": [
    "# cols = [\"dphi_1\",\"dphi_2\",\"dphi_3\",\"dphi_4\",\"dphi_5\",\"dphi_6\",\n",
    "#        \"dtheta_1\",\"dtheta_2\",\"dtheta_3\",\"dtheta_4\",\"dtheta_5\", \"dtheta_6\",\n",
    "#        \"bend_1\",\"bend_2\",\"bend_3\",\"bend_4\",\n",
    "#        \"track theta\"]\n",
    "\n",
    "# x = np.concatenate((x_train_displ,x_test_displ),axis=0)\n",
    "# y = np.concatenate((y_train_displ,y_test_displ),axis=0)\n",
    "# dxy = np.concatenate((dxy_train_displ,dxy_test_displ),axis=0)\n",
    "\n",
    "# corr_plot(x,y,dxy,columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ubbR2loeCbxL",
   "metadata": {
    "id": "ubbR2loeCbxL"
   },
   "source": [
    "### Part- III: Load and Account the Performance for the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1ba56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56b1ba56",
    "outputId": "9747728b-3bea-4552-9a52-98f68870b4e8"
   },
   "outputs": [],
   "source": [
    "baseline = loading_trained_model(filepath = \"./models/\",\n",
    "                                 model_filename = \"model\")\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nzWEKXR1Ctry",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "id": "nzWEKXR1Ctry",
    "outputId": "6b6a8920-c000-4201-907d-f4ad8993498a"
   },
   "outputs": [],
   "source": [
    "# Quantitative Evaluation\n",
    "k_fold_validation(model = baseline, \n",
    "          x = x_test_displ, \n",
    "          y = y_test_displ, \n",
    "          dxy = dxy_test_displ, \n",
    "          folds =1,\n",
    "          metric_type = \"MAE\")    \n",
    "k_fold_validation(model = baseline, \n",
    "          x = x_test_displ, \n",
    "          y = y_test_displ, \n",
    "          dxy = dxy_test_displ, \n",
    "          folds =1,\n",
    "          metric_type = \"RMSE\")   \n",
    "\n",
    "# Qualitative Evaluation:\n",
    "__generate_delta_plots__(model = baseline,\n",
    "                          x = x_test_displ,\n",
    "                          y = y_test_displ,\n",
    "                          dxy = dxy_test_displ,\n",
    "                          color = \"salmon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wwhNvFOpEe1m",
   "metadata": {
    "id": "wwhNvFOpEe1m"
   },
   "source": [
    "### Part-IV: Build the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630161ab",
   "metadata": {
    "id": "630161ab"
   },
   "outputs": [],
   "source": [
    "def run_iterative_pruning_v1(baseline_model = None,\n",
    "                          target_sparsity: float = 0.1, \n",
    "                          pruning_fraction_step:float = 0.1,\n",
    "                          training_params: list = []):\n",
    "    \"\"\"\n",
    "    Uses a new train-test-val split for each pruning cycle. \n",
    "    \"\"\"\n",
    "    \n",
    "    if target_sparsity > 1.0 or target_sparsity <= 0:\n",
    "        print(\"INVALID value entered for target sparsity, it can only be in the range [0,1]\")\n",
    "    if pruning_fraction_step > target_sparsity:\n",
    "        print(\"INVALID value entered for pruning fraction, it has to be <= target_sparsity\")\n",
    "    \n",
    "    # list of models, new pruned models get appended to it while training\n",
    "    pruned_models, training_history = [],[]\n",
    "    plot_colors = [\"red\",\"orange\",\"blue\",\"cyan\",\"purple\",\"green\",\"magenta\", \"salmon\"]\n",
    "    init_sparsity = pruning_fraction_step\n",
    "    i = 0\n",
    "    while(init_sparsity <= target_sparsity):\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "        print(\"Currently pruning the model upto {} % of the baseline\".format(round(init_sparsity*100)))\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        x_train_displ, x_test_displ, y_train_displ, y_test_displ, dxy_train_displ, dxy_test_displ =  muon_data_split(infile_muon_displ, \n",
    "                                                                                                                       reg_pt_scale=reg_pt_scale, \n",
    "                                                                                                                       reg_dxy_scale=reg_dxy_scale, \n",
    "                                                                                                                       test_size=0.315)\n",
    "        y_train_displ = np.abs(y_train_displ)\n",
    "        y_test_displ = np.abs(y_test_displ)\n",
    "\n",
    "        # training loop begins\n",
    "        lr = training_params[i]['lr']\n",
    "        clipnorm = training_params[i]['clipnorm']\n",
    "        eps = training_params[i]['eps']\n",
    "        momentum = training_params[i]['momentum']\n",
    "        retrain_epochs = training_params[i]['epochs']\n",
    "        retrain_batch_size = training_params[i]['batch_size']\n",
    "        l1_reg = training_params[i]['l1_reg']\n",
    "        l2_reg = training_params[i]['l2_reg']\n",
    "        sparsity = init_sparsity\n",
    "        \n",
    "        # define optimizer, callbacks here\n",
    "        adam = Adam(lr=lr, clipnorm=clipnorm)\n",
    "        lr_decay = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "        terminate_on_nan = TerminateOnNaN()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=18, \n",
    "                                       verbose=True,\n",
    "                                       mode='min')\n",
    "        curr_model = None\n",
    "        if len(pruned_models) == 0:\n",
    "            curr_model = baseline_model\n",
    "        else:\n",
    "            curr_model = pruned_models[-1]\n",
    "\n",
    "        pruned_model = create_sparse_model(model = curr_model,\n",
    "                                               input_dim = nvariables,\n",
    "                                               output_dim = 2,\n",
    "                                               k_sparsity = sparsity,\n",
    "                                               bn_epsilon = eps,\n",
    "                                               bn_momentum = momentum,\n",
    "                                               l1_reg = l1_reg,\n",
    "                                               l2_reg = l1_reg,\n",
    "                                               kernel_initializer=\"glorot_uniform\",\n",
    "                                               optimizer = adam)\n",
    "\n",
    "        pruned_model, history = train_sparse_model(sparse_model = pruned_model,\n",
    "                                                           x = x_train_displ,\n",
    "                                                           y = y_train_displ,\n",
    "                                                           dxy = dxy_train_displ,\n",
    "                                                           retrain_epochs = retrain_epochs,\n",
    "                                                           batch_size = retrain_batch_size,\n",
    "                                                           callbacks=[lr_decay, \n",
    "                                                                      early_stopping, \n",
    "                                                                      terminate_on_nan],\n",
    "                                                           verbose = True,\n",
    "                                                           validation_split=0.1)\n",
    "        \n",
    "        # evaluate the model\n",
    "        k_fold_validation(model = pruned_model, \n",
    "                  x = x_test_displ, \n",
    "                  y = y_test_displ, \n",
    "                  dxy = dxy_test_displ, \n",
    "                  folds =1,\n",
    "                  metric_type = \"MAE\")    \n",
    "        k_fold_validation(model = pruned_model, \n",
    "                  x = x_test_displ, \n",
    "                  y = y_test_displ, \n",
    "                  dxy = dxy_test_displ, \n",
    "                  folds =1,\n",
    "                  metric_type = \"RMSE\")   \n",
    "\n",
    "        __generate_delta_plots__(model = pruned_model,\n",
    "                                 x = x_test_displ,\n",
    "                                 y = y_test_displ,\n",
    "                                 dxy = dxy_test_displ,\n",
    "                                 color = plot_colors[i])\n",
    "        \n",
    "        pruned_models.append(pruned_model)\n",
    "        training_history.append(history)\n",
    "\n",
    "        # training ends\n",
    "        i += 1\n",
    "        init_sparsity += pruning_fraction_step\n",
    "    \n",
    "    return pruned_models, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "idzjce545Y3a",
   "metadata": {
    "id": "idzjce545Y3a"
   },
   "outputs": [],
   "source": [
    "def run_iterative_pruning_v2(X_train, y_train, dxy_train,\n",
    "                            X_test, y_test, dxy_test,\n",
    "                            baseline_model = None,\n",
    "                            init_sparsity: float = 0.1,\n",
    "                            target_sparsity: float = 1.0, \n",
    "                            pruning_fraction_step:float = 0.1,\n",
    "                            training_params: list = [],\n",
    "                            cv_folds: int = 50):\n",
    "    \"\"\"\n",
    "    Uses the same train-test-val-split for each pruning cycle.\n",
    "    \"\"\"\n",
    "        \n",
    "    if target_sparsity > 1.0 or target_sparsity <= 0:\n",
    "        print(\"INVALID value entered for target sparsity, it can only be in the range [0,1]\")\n",
    "    if pruning_fraction_step > target_sparsity:\n",
    "        print(\"INVALID value entered for pruning fraction, it has to be <= target_sparsity\")\n",
    "    \n",
    "    # list of models, new pruned models get appended to it while training\n",
    "    pruned_models, training_history = [],[]\n",
    "    plot_colors = [\"red\",\"orange\",\"blue\",\"cyan\",\"purple\",\"navy\",\"teal\",\"salmon\"]\n",
    "    i = 0\n",
    "    while(init_sparsity <= target_sparsity):\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "        print(\"Currently pruning the model upto {} % of the baseline\".format(round(init_sparsity*100)))\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # training loop begins\n",
    "        lr = training_params[i]['lr']\n",
    "        clipnorm = training_params[i]['clipnorm']\n",
    "        eps = training_params[i]['eps']\n",
    "        momentum = training_params[i]['momentum']\n",
    "        retrain_epochs = training_params[i]['epochs']\n",
    "        retrain_batch_size = training_params[i]['batch_size']\n",
    "        l1_reg = training_params[i]['l1_reg']\n",
    "        l2_reg = training_params[i]['l2_reg']\n",
    "        sparsity = init_sparsity\n",
    "        \n",
    "        # define optimizer, callbacks here\n",
    "        adam = Adam(lr=lr, clipnorm=clipnorm)\n",
    "        lr_decay = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "        terminate_on_nan = TerminateOnNaN()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=40, \n",
    "                                       verbose=True,\n",
    "                                       mode='min')\n",
    "        curr_model = None\n",
    "        if len(pruned_models) == 0:\n",
    "            curr_model = baseline_model\n",
    "        else:\n",
    "            curr_model = pruned_models[-1]\n",
    "\n",
    "        pruned_model = create_sparse_model(model = curr_model,\n",
    "                                               input_dim = nvariables,\n",
    "                                               output_dim = 2,\n",
    "                                               k_sparsity = sparsity,\n",
    "                                               bn_epsilon = eps,\n",
    "                                               bn_momentum = momentum,\n",
    "                                               l1_reg = l1_reg,\n",
    "                                               l2_reg = l1_reg,\n",
    "                                               kernel_initializer=\"glorot_uniform\",\n",
    "                                               optimizer = adam)\n",
    "\n",
    "        pruned_model, history = train_sparse_model(sparse_model = pruned_model,\n",
    "                                                           x = X_train,\n",
    "                                                           y = y_train,\n",
    "                                                           dxy = dxy_train,\n",
    "                                                           retrain_epochs = retrain_epochs,\n",
    "                                                           batch_size = retrain_batch_size,\n",
    "                                                           callbacks=[lr_decay, \n",
    "                                                                      early_stopping, \n",
    "                                                                      terminate_on_nan],\n",
    "                                                           verbose = True,\n",
    "                                                           validation_split=0.1)\n",
    "      \n",
    "        k_fold_validation(model = pruned_model, \n",
    "                        x = X_test, \n",
    "                        y = y_test, \n",
    "                        dxy = dxy_test, \n",
    "                        folds =cv_folds,\n",
    "                        metric_type = \"MAE\")    \n",
    "        k_fold_validation(model = pruned_model, \n",
    "                        x = X_test, \n",
    "                        y = y_test, \n",
    "                        dxy = dxy_test, \n",
    "                        folds =cv_folds,\n",
    "                        metric_type = \"RMSE\")   \n",
    "\n",
    "        __generate_delta_plots__(model = pruned_model,\n",
    "                                x = X_test,\n",
    "                                y = y_test,\n",
    "                                dxy = dxy_test,\n",
    "                                color = plot_colors[i])\n",
    "        \n",
    "        pruned_models.append(pruned_model)\n",
    "        training_history.append(history)\n",
    "\n",
    "        # training ends\n",
    "        i += 1\n",
    "        init_sparsity += pruning_fraction_step\n",
    "    \n",
    "    return pruned_models, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a61c8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "31a61c8a",
    "outputId": "704cddeb-4d06-4e47-e000-5ee905ff3fb1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pruned_models, training_history = run_iterative_pruning(\n",
    "#                                                         baseline_model = baseline,\n",
    "#                                                         target_sparsity = 0.7,\n",
    "#                                                         training_params = ft_params)\n",
    "pruned_models, training_history = run_iterative_pruning_v2(x_train_displ, y_train_displ, dxy_train_displ,\n",
    "                                                          x_test_displ, y_test_displ, dxy_test_displ,\n",
    "                                                          baseline_model = baseline,\n",
    "                                                          init_sparsity = 0.1,\n",
    "                                                          target_sparsity = 0.8,\n",
    "                                                          pruning_fraction_step = 0.1,\n",
    "                                                          training_params = ft_params,\n",
    "                                                          cv_folds = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83027a",
   "metadata": {
    "id": "8d83027a"
   },
   "source": [
    "### Part- V: Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a376f3",
   "metadata": {
    "id": "44a376f3"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pruned_models)):\n",
    "    with open(\"./trainingLog\" + str((i+1)*10), 'wb') as file_pi:\n",
    "          pickle.dump(training_history[i].history, file_pi)\n",
    "    model_filename = \"custom_model_\" + str((i+1)*10)\n",
    "    saving_pruned_model(model = pruned_models[i], \n",
    "                      filepath= sys.path[-1] + \"/models\", \n",
    "                      model_filename = model_filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "implementing_iterative_pruning-v2.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
